{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b534a64-1263-4607-9946-b61af176eab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esto es para que la librería se actualice automáticamente (el .py de los includes en este caso), en producción hay que tomar la decisión si debería quitarse para evitar cualquier problema y hacer un restart si se cambia la librería o si dejarlo y cuando se haga un cambio, lo va a tomar automáticamente\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be0aaab-7cf0-4c2b-b77a-52c260819f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install msal requests #PENDING esto debería ir en el cluster\n",
    "%pip install boto3 #PENDING esto debería ir en el cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c1a607-f131-48ff-8372-ab88f69b04cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from msal import ConfidentialClientApplication\n",
    "from datetime import datetime\n",
    "from io import StringIO, BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de includes y de config\n",
    "from includes.file_functions import check_filename, imprimir \n",
    "from includes import control_functions\n",
    "import config.config as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c03aa4-6b9e-4e2c-90c3-f845e8ea6a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ad7baf-3d1d-4396-be1d-a6c549663a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG START\n",
    "\n",
    "process_setup_name = 'Load BI OVC'\n",
    "process_setup_step_name = 'sharepoint to s3'\n",
    "sys_modified_by_name = 'NBK - Load Finance - SP to S3'\n",
    "source_system_code = 'SAP1C'\n",
    "\n",
    "process_run_id = control_functions.log_process_run_start(process_setup_name,process_setup_step_name,source_system_code,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625e0652-4f15-4b59-b695-c4302462c938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET DATA FROM CONTROL TABLE\n",
    "df = control_functions.get_process_setup_parameters(process_setup_name,process_setup_step_name)\n",
    "\n",
    "sourceFileNamePrefix = df.select('process_setup_source_file_name').collect()[0][0]\n",
    "sourceFileExtension = df.select('process_setup_source_file_extension').collect()[0][0]\n",
    "sourceFileNameMask = df.select('process_setup_source_file_name_mask').collect()[0][0]\n",
    "raw_bucket = df.select('process_setup_target_bucket_name').collect()[0][0]\n",
    "bucketFolderKey = df.select('process_setup_target_bucket_folder_key').collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbf6ae9-2a63-4b32-8081-1cf4cf37f19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. Configuración\n",
    "# ======================\n",
    "\n",
    "tenant_id = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"tenant_id\")\n",
    "client_id = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"client_secret\")\n",
    "\n",
    "hostname = cfg.hostname\n",
    "site_relative = cfg.site_relative\n",
    "\n",
    "# ======================\n",
    "# 2. Obtener token (Bearer)\n",
    "# ======================\n",
    "authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "scope = [\"https://graph.microsoft.com/.default\"]\n",
    "\n",
    "app = ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
    "token = app.acquire_token_for_client(scopes=scope)\n",
    "\n",
    "if \"access_token\" not in token: \n",
    "    raise Exception(f\"Error getting token: {token}\") \n",
    "access_token = token[\"access_token\"]\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "# ======================\n",
    "# 3. Obtener siteId\n",
    "# ======================\n",
    "\n",
    "url_site = f\"https://graph.microsoft.com/v1.0/sites/{hostname}:/{site_relative}\"\n",
    "r = requests.get(url_site, headers=headers)\n",
    "\n",
    "#print(json.dumps(r, indent=2))\n",
    "\n",
    "site = r.json()\n",
    "site_id = site.get(\"id\")\n",
    "\n",
    "# ======================\n",
    "# 4. Obtener drives y elegir \"Documents\"\n",
    "# ======================\n",
    "url_drives = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives\"\n",
    "drives_response = requests.get(url_drives, headers=headers).json()\n",
    "\n",
    "if 'value' not in drives_response:\n",
    "    raise Exception(f\"Error fetching drives: {drives_response}\")\n",
    "drives = drives_response\n",
    "\n",
    "drive_id = next(d[\"id\"] for d in drives[\"value\"] if d[\"name\"] == cfg.sp_file_folder)\n",
    "\n",
    "# ======================\n",
    "# 5. Obtener archivos en carpeta\n",
    "# ======================\n",
    "\n",
    "folder = \"BI_OVC_TST\" #PENDING charlar dejar acá o más arriba, porque solo se usa aquí\n",
    "url_children = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/root:/{folder}:/children\"\n",
    "resp = requests.get(url_children, headers=headers).json()\n",
    "\n",
    "file_prefix = sourceFileNamePrefix\n",
    "files = [f for f in resp.get(\"value\", []) if f.get(\"name\", \"\").lower().startswith(file_prefix.lower())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e47756-e940-4c1b-996b-eaca0cd30e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 6. Obtener archivos\n",
    "# ===============================================\n",
    "\n",
    "files_matched = []\n",
    "\n",
    "#Only get files matching the mask\n",
    "for f in files:\n",
    "    name = f.get(\"name\", \"\")\n",
    "\n",
    "    if check_filename(sourceFileNameMask,sourceFileNamePrefix.lower(),sourceFileExtension.lower(),name.lower()):\n",
    "        files_matched.append(f)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0716de9d-a7be-455a-85f3-7602fbb72a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 7. Mover de Sharepoint a S3\n",
    "# ======================\n",
    "\n",
    "s3_bucket = raw_bucket \n",
    "s3_base_path = bucketFolderKey\n",
    "\n",
    "logs = []\n",
    "\n",
    "# Inicializar cliente de S3\n",
    "\n",
    "# Obtiene la región por defecto del entorno\n",
    "session = boto3.session.Session()\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"aws_access_key_id\"),\n",
    "    aws_secret_access_key=dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"aws_secret_access_key\"),\n",
    "    region_name=session.region_name\n",
    ")\n",
    "\n",
    "for f in files_matched:\n",
    "    file_name = f.get(\"name\")\n",
    "    download_url = f.get(\"@microsoft.graph.downloadUrl\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not file_name or not download_url:\n",
    "        print(f\"⚠️ FILE NOT LOADED - PLEASE CHECK: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    target_file_name = f\"{file_name.rsplit('.',1)[0]}.{file_name.rsplit('.',1)[1]}\" #nombre original\n",
    "    target_s3_key = f\"{s3_base_path}/{target_file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Descargar archivo en memoria\n",
    "        response = requests.get(download_url)\n",
    "        response.raise_for_status()\n",
    "        file_bytes = response.content\n",
    "\n",
    "        # Subir a S3 directamente como binario\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=target_s3_key, Body=file_bytes)\n",
    "        print(f\"✅ File Loaded: s3://{s3_bucket}/{target_s3_key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR LOADING FILE {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead7d09b-7274-442e-943a-b1f262f48632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG END\n",
    "control_functions.log_process_run_end(process_run_id,sys_modified_by_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Finance - SP to S3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
