{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "495c8770-7e17-490c-b6d4-c3701ce26fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esto es para que la librería se actualice automáticamente (el .py de los includes en este caso), en producción hay que tomar la decisión si debería quitarse para evitar cualquier problema y hacer un restart si se cambia la librería o si dejarlo y cuando se haga un cambio, lo va a tomar automáticamente\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c1a607-f131-48ff-8372-ab88f69b04cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import sys\n",
    "import importlib\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de cada carpeta\n",
    "from includes import control_functions\n",
    "from includes import validations\n",
    "from schema import fin_act_sch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c03aa4-6b9e-4e2c-90c3-f845e8ea6a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba9a1ffa-f884-471c-b82d-f172edc460f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GENERIC PARAMETER\n",
    "sys_status_column_name = 'sys_status_code'\n",
    "error_status_code = 'E'\n",
    "process_source_name = 'Load BI OVC'\n",
    "process_step_name = 'raw to bronze'\n",
    "sys_modified_by_name = 'NBK - Load Finance - raw to bronze'\n",
    "source_system_code = 'sapbr'\n",
    "fn_status = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1fb259-1a50-4673-a3b2-b1502e495e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG START\n",
    "process_run_id, fn_status = control_functions.log_process_run_start(process_source_name,process_step_name,source_system_code,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d82cb70-f8cd-43ce-9fb1-95bfc1beda5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IF errors, set row status to E\n",
    "if process_run_id > 0:\n",
    "  control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "\n",
    "if fn_status == False:\n",
    "  print(f\"❌ ERROR STARTING PROCESS\")\n",
    "  raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55048428-5d27-4d51-b138-4bd3dd2be808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#FILE PARAMETERS FROM NOTEBOOK\n",
    "sourceFileNamePrefix = 'tdf_fin_variable_cost_act_sapbr'\n",
    "sourceFileExtension = 'txt'\n",
    "sourceFileDelimiter = '\\\\t'\n",
    "sourceFileEncoding = 'ISO-8859-1'\n",
    "sourceSchema = 'fin-act-raw-sapbr'\n",
    "tableName = 'tb_fin_variable_cost_act_sapbr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546128e8-5974-4197-abcf-02752359426c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET DATA FROM CONTROL TABLE\n",
    "try:\n",
    "    df = control_functions.get_process_source_parameters(process_source_name,process_step_name)\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR GETTING PROCESS SETUP PARAMETERS: {e}\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "079041b5-ca9e-4eb7-bf42-3203806a4902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PARAMETERS FROM CONTROL TABLE\n",
    "SourceBucket = df.select('source_bucket_name').collect()[0][0]\n",
    "SourceBucketFolderKey = df.select('source_bucket_folder_key').collect()[0][0]\n",
    "TargetBucket = df.select('target_bucket_name').collect()[0][0]\n",
    "TargetBucketFolderKey = df.select('target_bucket_folder_key').collect()[0][0]\n",
    "ArchiveBucket = df.select('archive_bucket_name').collect()[0][0]\n",
    "ArchiveBucketFolderKey = df.select('archive_bucket_folder_key').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fafbec-5e16-4646-9400-14d37bdadf59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ SCHEMAS - from .py\n",
    "try:\n",
    "    raw_schema = fin_act_sch.get_schema(sourceSchema)\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR READING SOURCE SCHEMA: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3df8ac3-993b-4640-81bd-29ec759f037c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET FILE NAMES FROM RAW SOURCE\n",
    "files_to_process = []\n",
    "\n",
    "raw_generic_path = \"s3://\" + SourceBucket + \"/\" + SourceBucketFolderKey + \"/\"\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(raw_generic_path)\n",
    "\n",
    "    files_to_process = [\n",
    "        f for f in files\n",
    "        if f.name.lower().startswith(sourceFileNamePrefix.lower()) and f.name.lower().endswith(f\".{sourceFileExtension.lower()}\")\n",
    "    ]\n",
    "\n",
    "    files_to_process.sort()\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"⚠️ NO MATCHING FILES\")\n",
    "\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR GETTING FILE NAME FROM SOURCE: {e}\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3b58a9-9618-416e-8b5e-5a0cf3e839b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "archive_path = \"s3://\" + ArchiveBucket + \"/\" + ArchiveBucketFolderKey\n",
    "bronze_generic_path = \"s3://\" + TargetBucket + \"/\" \n",
    "\n",
    "source_cnt = 0\n",
    "target_cnt = 0\n",
    "curr_row_flg_updated = 0 \n",
    "\n",
    "if files_to_process:\n",
    "    for i, file in enumerate(files_to_process, start=1):\n",
    "        #CHECK FILENAME\n",
    "\n",
    "        #READ SOURCE FILE AND ADD COLUMN NAMES TO DATAFRAME\n",
    "        file_path = raw_generic_path + file.name\n",
    "\n",
    "        try: \n",
    "            df_raw = spark.read.options(encoding=sourceFileEncoding,delimiter=sourceFileDelimiter, header=False, schema=raw_schema).csv(file_path) \n",
    "        \n",
    "        except Exception as e:\n",
    "            control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "            print(f\"❌ ERROR READING FILE {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        #Rename columns using schema names\n",
    "        df_raw = df_raw.toDF(*[f.name for f in raw_schema.fields])\n",
    "\n",
    "        #Add DW_ROW_ID to be able to identify rows in case of errors in validations\n",
    "        window_spec = Window.orderBy(F.lit(1))\n",
    "\n",
    "        # Composite ID: process_run_id + \"_\" + seq nbr\n",
    "        df_raw = df_raw.withColumn(\n",
    "            \"DW_ROW_ID\",\n",
    "            F.concat(F.lit(str(process_run_id) + \"_\"), F.row_number().over(window_spec))\n",
    "        )\n",
    "\n",
    "        #Accumulate count of rows from source before any validation, count inside loop in case of many files loaded\n",
    "        source_cnt += df_raw.count()\n",
    "\n",
    "        #PH VALIDATIONS\n",
    "\n",
    "        #Get all PH validations that applies for the current process\n",
    "        try:\n",
    "            df_validations = validations.get_object_validation(process_run_id, 'PH')\n",
    "\n",
    "        except Exception as e:\n",
    "            control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "            print(f\"❌ ERROR GETTING OBJECT VALIDATIONS LIST: {e}\")\n",
    "            raise\n",
    "\n",
    "        #Validate and get records validated and records rejected\n",
    "        try:\n",
    "            df_bronze, df_bronze_rejected = validations.physical_validation(df_raw, df_validations, process_run_id)\n",
    "            \n",
    "        except Exception as e:\n",
    "            control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "            print(f\"❌ ERROR IN PHYSICAL VALIDATIONS FN: {e}\")\n",
    "            raise\n",
    "\n",
    "        #WRITE IN BRONZE\n",
    "        #if df_bronze and df_bronze.limit(1).count() > 0:\n",
    "        if df_bronze.limit(1).count() > 0:\n",
    "            bronze_delta_path = bronze_generic_path.rstrip(\"/\") + \"/\" + tableName \n",
    "\n",
    "            try:\n",
    "                dbutils.fs.ls(bronze_delta_path)\n",
    "            except:\n",
    "                #dbutils.fs.mkdirs(bronze_delta_path)\n",
    "                df_empty = spark.createDataFrame([], \"DW_VALID_FROM_DT timestamp\")  # ajusta el esquema\n",
    "                df_empty.write.format(\"delta\").save(bronze_delta_path)\n",
    "                \n",
    "            try:\n",
    "                # Enrich DF ALWAYS before writing\n",
    "                df_bronze = df_bronze.withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")) \\\n",
    "                    .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn('DW_CURR_ROW_FLG', F.lit(True)) \\\n",
    "                    .withColumn('DW_FILE_NAME', F.lit(file.name))\n",
    "                \n",
    "                if DeltaTable.isDeltaTable(spark, bronze_delta_path):\n",
    "                    delta_table = DeltaTable.forPath(spark, bronze_delta_path)\n",
    "                    \n",
    "                    # Update existing records (only the first time (i==1), so doesnt mark as False the records of other files inserted in the same process running).\n",
    "                    if i == 1 and curr_row_flg_updated == 0 and \"DW_CURR_ROW_FLG\" in delta_table.toDF().columns:\n",
    "                        delta_table.update(\n",
    "                            condition=\"DW_CURR_ROW_FLG = true\",\n",
    "                            set={\n",
    "                                \"DW_CURR_ROW_FLG\": \"false\",\n",
    "                                \"DW_VALID_TO_DT\": F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        curr_row_flg_updated = 1 \n",
    "                    mode = \"append\"\n",
    "                else:\n",
    "                    mode = \"overwrite\"  #first loading\n",
    "                \n",
    "                # DataFrame with only the schema\n",
    "                df_bronze.write.format(\"delta\").mode(mode).option(\"mergeSchema\", \"true\").save(bronze_delta_path)\n",
    "            \n",
    "            except Exception as e:\n",
    "                control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "                print(f\"❌ ERROR READING FILE {bronze_generic_path}: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"⚠️ EMPTY BRONZE DATAFRAME\")\n",
    "\n",
    "        #WRITE IN BRONZE_REJECTED\n",
    "        #if df_bronze_rejected and df_bronze_rejected.limit(1).count() > 0:\n",
    "        df_empty = []\n",
    "        if df_bronze_rejected.limit(1).count() > 0:\n",
    "            bronze_delta_path = bronze_generic_path.rstrip(\"/\") + \"-rejected\" + \"/\" + tableName \n",
    "\n",
    "            try:\n",
    "                dbutils.fs.ls(bronze_delta_path)\n",
    "            except:\n",
    "                df_empty = spark.createDataFrame([], \"DW_VALID_FROM_DT timestamp\")  # adjust schema\n",
    "                df_empty.write.format(\"delta\").save(bronze_delta_path)\n",
    "                \n",
    "            try:\n",
    "                # Enrich DF ALWAYS before writing\n",
    "                df_bronze_rejected = df_bronze_rejected.withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")) \\\n",
    "                    .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn('DW_CURR_ROW_FLG', F.lit(True)) \\\n",
    "                    .withColumn('DW_FILE_NAME', F.lit(file.name))\n",
    "                \n",
    "                if DeltaTable.isDeltaTable(spark, bronze_delta_path):\n",
    "                    delta_table_rejected = DeltaTable.forPath(spark, bronze_delta_path)\n",
    "                    \n",
    "                    # Update existing records (only the first time (i==1), so doesnt mark as False the records of other files inserted in the same process running) And if it wasn't updated for Write in Bronze first\n",
    "\n",
    "                    if i == 1 and curr_row_flg_updated == 0 and \"DW_CURR_ROW_FLG\" in delta_table_rejected.toDF().columns:\n",
    "                        delta_table_rejected.update(\n",
    "                            condition=\"DW_CURR_ROW_FLG = true\",\n",
    "                            set={\n",
    "                                \"DW_CURR_ROW_FLG\": \"false\",\n",
    "                                \"DW_VALID_TO_DT\": F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")\n",
    "                            }\n",
    "                        )\n",
    "                    \n",
    "                    curr_row_flg_updated = 1 \n",
    "                    \n",
    "                    mode = \"append\"\n",
    "                else:\n",
    "                    mode = \"overwrite\"  # primera carga\n",
    "                \n",
    "                # DataFrame mínimo solo con esquema\n",
    "\n",
    "                df_bronze_rejected.write.format(\"delta\").mode(mode).option(\"mergeSchema\", \"true\").save(bronze_delta_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "                print(f\"❌ ERROR WRITING FILE {bronze_generic_path}: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(\"⚠️ EMPTY BRONZE REJECTED DATAFRAME\")\n",
    "\n",
    "        #Accumulate count of rows moved to target (not rejected), count inside loop in case of many files loaded\n",
    "        target_cnt += df_bronze.count()\n",
    "\n",
    "        #Always move to archive\n",
    "        source_path = raw_generic_path.rstrip(\"/\")\n",
    "        target_path = archive_path.rstrip(\"/\")\n",
    "\n",
    "        # Ruta completa del archivo origen\n",
    "        source_file_full = f\"{source_path}/{file.name}\"\n",
    "\n",
    "        # Verificar existencia\n",
    "        try:\n",
    "            files = [f.name for f in dbutils.fs.ls(source_path)]\n",
    "            if file.name not in files:\n",
    "                print(f\"⚠️ FILE NOT FOUND: {source_file_full}\")\n",
    "        except Exception as e:\n",
    "            control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "            print(f\"❌ ERROR ACCESSING SOURCE PATH: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Crear carpeta destino si no existe\n",
    "        try:\n",
    "            dbutils.fs.ls(target_path)\n",
    "        except:\n",
    "            dbutils.fs.mkdirs(target_path)\n",
    "\n",
    "        # Generar nombre con timestamp\n",
    "        target_file_name = file.name\n",
    "\n",
    "        # Rutas completas\n",
    "        target_file_full = f\"{target_path}/{target_file_name}\"\n",
    "\n",
    "        # Copiar y borrar \n",
    "        #DESCOMENTAR PARA MOVER A ARCHIVO\n",
    "        #dbutils.fs.cp(source_file_full, target_file_full)\n",
    "        #dbutils.fs.rm(source_file_full)\n",
    "\n",
    "        print(f\"✅ File Archived: {source_file_full} → {target_file_full}\")\n",
    "\n",
    "    #LOG SOURCE & TARGET RECORD COUNT\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,'process_run_source_record_count', source_cnt)\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,'process_run_target_record_count', target_cnt)\n",
    "else:\n",
    "    print(f\"⚠️ NO FILES TO PROCESS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45346a66-9d16-4291-bba7-b705d1baa8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#table = \"`latam-md-finance`.bronze.tb_fin_variable_cost_act_sapbr\" \n",
    "#spark.sql(\"DROP TABLE IF EXISTS \" + table + \"\")\n",
    "#spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table + \" USING DELTA LOCATION 's3a://latam-md-finance-bronze/tb_fin_variable_cost_act_sapbr'\")\n",
    "\n",
    "#print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae1a3c4-3864-468e-af51-762677338721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG END\n",
    "x = control_functions.log_process_run_end(process_run_id,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e1cab4-b577-41fe-bda8-64743d7a58b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IF errors, set row status to E\n",
    "if x == False:\n",
    "  control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5174651618437435,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Finance - raw to bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
