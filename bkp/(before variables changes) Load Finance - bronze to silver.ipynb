{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a178f270-4ab5-447e-b8c7-d4b083417982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esto es para que la librería se actualice automáticamente (el .py de los includes en este caso), en producción hay que tomar la decisión si debería quitarse para evitar cualquier problema y hacer un restart si se cambia la librería o si dejarlo y cuando se haga un cambio, lo va a tomar automáticamente\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d34753f-5f14-47ef-9d26-7713e1b0afd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import sys\n",
    "import importlib\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de cada carpeta\n",
    "from includes import control_functions\n",
    "from includes import validations\n",
    "from schema import fin_act_sch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8018f497-49b7-41d4-b4eb-c074da317b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#CREATE PYSPARK SESSION\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d023a81f-2528-4636-861c-0f475b7379d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GENERIC PARAMETER\n",
    "sys_status_column_name = 'sys_status_code'\n",
    "error_status_code = 'E'\n",
    "process_setup_name = 'Load BI OVC'\n",
    "process_setup_step_name = 'bronze to silver'\n",
    "sys_modified_by_name = 'NBK - Load Finance - bronze to silver'\n",
    "source_system_code = 'sapbr'\n",
    "fn_status = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4fa199-1b97-4982-a783-6b671a3934c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG START\n",
    "process_run_id, fn_status = control_functions.log_process_run_start(process_setup_name,process_setup_step_name,source_system_code,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3475d5a5-6aa8-4e29-95ed-dac7d1a9114a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IF errors, set row status to E\n",
    "if process_run_id > 0:\n",
    "  control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "if fn_status == False:\n",
    "  print(f\"❌ ERROR STARTING PROCESS\")\n",
    "  raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6fb491-c24e-40c7-913d-d8fb2d4b9383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET DATA FROM CONTROL TABLE\n",
    "try:\n",
    "    df = control_functions.get_process_setup_parameters(process_setup_name,process_setup_step_name)\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR GETTING PROCESS SETUP PARAMETERS: {e}\")\n",
    "    raise \n",
    "\n",
    "#sourceSchema = df.select('process_setup_source_data_definition').collect()[0][0]\n",
    "SourceBucket = df.select('process_setup_source_bucket_name').collect()[0][0]\n",
    "SourceBucketFolderKey = df.select('process_setup_source_bucket_folder_key').collect()[0][0]\n",
    "#targetSchema = df.select('process_setup_target_data_definition').collect()[0][0]\n",
    "TargetBucket = df.select('process_setup_target_bucket_name').collect()[0][0]\n",
    "TargetBucketFolderKey = df.select('process_setup_target_bucket_folder_key').collect()[0][0]\n",
    "targetTable = df.select('process_setup_target_table_name').collect()[0][0]\n",
    "#sourceTable = df.select('process_setup_source_table_name').collect()[0][0]\n",
    "#sourceLayer = df.select('process_setup_source_layer').collect()[0][0]\n",
    "#targetLayer = df.select('process_setup_target_layer').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9eee722-9b78-4e1f-a770-f159aef86c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ SCHEMAS - from .py\n",
    "\"\"\"try:\n",
    "    src_schema = fin_act_sch.get_schema(sourceSchema)\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR READING SOURCE SCHEMA: {e}\")\n",
    "    raise \n",
    "\n",
    "try:\n",
    "    trgt_schema = fin_act_sch.get_schema(targetSchema)\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR READING TARGET SCHEMA: {e}\")\n",
    "    raise \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814864e2-590a-4583-bec3-a4a83ce01b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get current data from Bronze\n",
    "source_path = \"s3://\" + SourceBucket + \"/\" + SourceBucketFolderKey + \"/\"\n",
    "df_brz = []\n",
    "\n",
    "try:\n",
    "    # check file existense in DBFS\n",
    "    dbutils.fs.ls(source_path)\n",
    "\n",
    "    # read table\n",
    "    df_brz = spark.read.format(\"delta\").load(source_path)\n",
    "\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ CAN'T READ DELTA {source_path}: {e}\")\n",
    "    raise\n",
    "\n",
    "df_brz = df_brz.where(F.col('DW_CURR_ROW_FLG') == F.lit(True)) #get current\n",
    "df_brz = df_brz.drop('DW_VALID_FROM_DT','DW_VALID_TO_DT') #drop this fields as in silver will have the current date\n",
    "df_brz = df_brz.withColumn(\"POSTING_MTH_ID\",F.date_format(F.to_date(\"POSTING_DATE\", \"yyyyMMdd\"), \"yyyyMM\"))\n",
    "\n",
    "#get last version available per month (this is a business definition)\n",
    "df_brz_last_version = df_brz.groupBy(\"POSTING_MTH_ID\").agg(\n",
    "    F.max(\"DW_FILE_NAME\").alias(\"DW_FILE_NAME\")\n",
    ")\n",
    "\n",
    "#display(df_brz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2f9a2c-2f58-4b35-a7e9-122ff71d6fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET LAST DATA AVAILABLE \n",
    "\n",
    "df_brz = (\n",
    "    df_brz.join(\n",
    "        df_brz_last_version,\n",
    "        on=[\"DW_FILE_NAME\", \"POSTING_MTH_ID\"],  # union key\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "#drop DW_ROW_ID column, to create the newone for silver (previous ids are from bronze)\n",
    "df_brz = df_brz.drop(\"DW_ROW_ID\")\n",
    "\n",
    "#Add DW_ROW_ID to be able to identify rows in case of errors in validations\n",
    "window_spec = Window.orderBy(F.lit(1))\n",
    "\n",
    "# Composite ID: process_run_id + \"_\" + seq nbr\n",
    "df_brz = df_brz.withColumn(\n",
    "    \"DW_ROW_ID\",\n",
    "    F.concat(F.lit(f\"{process_run_id}_\"), F.row_number().over(window_spec).cast(\"string\"))\n",
    ")\n",
    "\n",
    "#display(df_brz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909b9746-8ee8-4ee4-b0e2-c27cd4f25940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#BS VALIDATIONS\n",
    "source_cnt = 0\n",
    "target_cnt = 0\n",
    "\n",
    "#Count of rows from source before any validation\n",
    "source_cnt = df_brz.count()\n",
    "\n",
    "#Get all BS validations that applies for the current process\n",
    "try:\n",
    "    df_validations = validations.get_object_validation(process_run_id, 'BS')\n",
    "    \n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR GETTING OBJECT VALIDATIONS LIST: {e}\")\n",
    "    raise\n",
    "\n",
    "#Validate and get records validated and records rejected\n",
    "try:\n",
    "    df_validated = validations.business_validation(df_brz, df_validations, process_run_id)\n",
    "\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR IN BUSINESS VALIDATIONS FN: {e}\")\n",
    "    raise\n",
    "\n",
    "#Count of rows to insert in target after validations\n",
    "target_cnt = df_validated.count()\n",
    "\n",
    "#display(df_validated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d974a1a8-f6a1-4b58-889b-0169793cf679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_validated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbdd7bd1-dc37-486b-8690-74ba58e41816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#WRITE IN SILVER\n",
    "\n",
    "if df_validated.limit(1).count() > 0:\n",
    "    target_path = \"s3://\" + TargetBucket + \"/\" + TargetBucketFolderKey + \"/\"\n",
    "\n",
    "    #check if target path exists, if not will create it\n",
    "    try:\n",
    "        dbutils.fs.ls(target_path)\n",
    "    except:\n",
    "        dbutils.fs.mkdirs(target_path)\n",
    "\n",
    "    df_silver = []\n",
    "\n",
    "    # check if target is an existing delta table, if not will create it\n",
    "    if DeltaTable.isDeltaTable(spark, target_path):\n",
    "\n",
    "        delta_table = DeltaTable.forPath(spark, target_path)\n",
    "\n",
    "        # get all current records from silver already loaded\n",
    "        df_silver = (\n",
    "            spark.read.format(\"delta\").load(target_path)\n",
    "            .filter(F.col('DW_CURR_ROW_FLG') == F.lit(True))\n",
    "            .drop('DW_VALID_FROM_DT','DW_VALID_TO_DT')\n",
    "        )\n",
    "        #generate Posting Month ID for reprocess\n",
    "        df_silver = df_silver.withColumn(\"POSTING_MTH_ID\",F.date_format(F.to_date(\"POSTING_DATE\", \"yyyyMMdd\"), \"yyyyMM\"))\n",
    "\n",
    "        #from the current, get those to be reprocessed to mark them as false\n",
    "        df_silver_reprocess = (\n",
    "            df_silver.join(\n",
    "                df_brz_last_version,\n",
    "                on=[\"POSTING_MTH_ID\"],  # key for union\n",
    "                how=\"inner\"\n",
    "            )\n",
    "        ).select(\"POSTING_MTH_ID\").distinct()\n",
    "        \n",
    "        #Mark records to reprocess as false\n",
    "        delta_table.alias(\"tgt\").merge(\n",
    "            df_silver_reprocess.alias(\"src\"),\n",
    "            \"tgt.POSTING_MTH_ID = src.POSTING_MTH_ID\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"tgt.DW_CURR_ROW_FLG = true\",\n",
    "            set={\n",
    "                \"DW_CURR_ROW_FLG\": F.lit(False),\n",
    "                \"DW_VALID_TO_DT\": F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")\n",
    "            }\n",
    "        ).execute()\n",
    "        \n",
    "        #Add DW columns to the validated data\n",
    "        df_validated.withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")) \\\n",
    "            .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\")) \\\n",
    "            .withColumn('DW_CURR_ROW_FLG', F.lit(True))\n",
    "\n",
    "        df_validated.write.format(\"delta\").mode(\"append\").save(target_path)\n",
    "\n",
    "    else:\n",
    "        #Create initial Delta table with DW columns\n",
    "        new_data = (\n",
    "            df_validated\n",
    "            .withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\"))\n",
    "            .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\"))\n",
    "            .withColumn('DW_CURR_ROW_FLG', F.lit(True))\n",
    "        )\n",
    "        # Grabar como nueva Delta Table\n",
    "        new_data.write.format(\"delta\").mode(\"append\").save(target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3241f06-9067-4d02-9965-60a7b7a27827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG SOURCE & TARGET RECORD COUNT\n",
    "control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,'process_run_source_record_count', source_cnt)\n",
    "control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,'process_run_target_record_count', target_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45e46e5-561c-4a6b-b88e-47a995066b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG END\n",
    "x = control_functions.log_process_run_end(process_run_id,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f86024-544f-4be3-8afe-3870d41ab223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IF errors, set row status to E\n",
    "if x == False:\n",
    "  control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d68cbf5-f130-4a8a-8ee9-3d858b3b00cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#table = \"`latam-md-finance`.silver_rejected.tb_fin_variable_cost_act\" \n",
    "#spark.sql(\"DROP TABLE IF EXISTS \" + table + \"\")\n",
    "#spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table + \" USING DELTA LOCATION 's3a://latam-md-finance-silver-rejected/tb_fin_variable_cost_act'\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8585516552376328,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(before variables changes) Load Finance - bronze to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
