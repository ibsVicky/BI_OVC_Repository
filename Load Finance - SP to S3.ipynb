{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b534a64-1263-4607-9946-b61af176eab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esto es para que la librería se actualice automáticamente (el .py de los includes en este caso), en producción hay que tomar la decisión si debería quitarse para evitar cualquier problema y hacer un restart si se cambia la librería o si dejarlo y cuando se haga un cambio, lo va a tomar automáticamente\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be0aaab-7cf0-4c2b-b77a-52c260819f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install msal requests #PENDING esto debería ir en el cluster\n",
    "%pip install boto3 #PENDING esto debería ir en el cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c1a607-f131-48ff-8372-ab88f69b04cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from msal import ConfidentialClientApplication\n",
    "from datetime import datetime\n",
    "from io import StringIO, BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de includes y de config\n",
    "from includes.file_functions import check_filename, imprimir \n",
    "import config.config as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c03aa4-6b9e-4e2c-90c3-f845e8ea6a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbf6ae9-2a63-4b32-8081-1cf4cf37f19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. Configuración\n",
    "# ======================\n",
    "\n",
    "tenant_id = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"tenant_id\")\n",
    "client_id = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"client_secret\")\n",
    "\n",
    "hostname = cfg.hostname\n",
    "site_relative = cfg.site_relative\n",
    "\n",
    "# ======================\n",
    "# 2. Obtener token (Bearer)\n",
    "# ======================\n",
    "authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "scope = [\"https://graph.microsoft.com/.default\"]\n",
    "\n",
    "app = ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
    "token = app.acquire_token_for_client(scopes=scope)\n",
    "\n",
    "if \"access_token\" not in token: \n",
    "    raise Exception(f\"Error getting token: {token}\") \n",
    "access_token = token[\"access_token\"]\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "# ======================\n",
    "# 3. Obtener siteId\n",
    "# ======================\n",
    "\n",
    "url_site = f\"https://graph.microsoft.com/v1.0/sites/{hostname}:/{site_relative}\"\n",
    "r = requests.get(url_site, headers=headers)\n",
    "\n",
    "#print(json.dumps(r, indent=2))\n",
    "\n",
    "site = r.json()\n",
    "site_id = site.get(\"id\")\n",
    "\n",
    "# ======================\n",
    "# 4. Obtener drives y elegir \"Documents\"\n",
    "# ======================\n",
    "url_drives = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives\"\n",
    "drives_response = requests.get(url_drives, headers=headers).json()\n",
    "\n",
    "if 'value' not in drives_response:\n",
    "    raise Exception(f\"Error fetching drives: {drives_response}\")\n",
    "drives = drives_response\n",
    "\n",
    "drive_id = next(d[\"id\"] for d in drives[\"value\"] if d[\"name\"] == cfg.sp_file_folder)\n",
    "\n",
    "# ======================\n",
    "# 5. Obtener archivos en carpeta\n",
    "# ======================\n",
    "\n",
    "folder = \"BI_OVC_TST\" #PENDING charlar dejar acá o más arriba, porque solo se usa aquí\n",
    "url_children = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/root:/{folder}:/children\"\n",
    "resp = requests.get(url_children, headers=headers).json()\n",
    "\n",
    "file_prefix = cfg.fin_var_cost_act_file_name\n",
    "files = [f for f in resp.get(\"value\", []) if f.get(\"name\", \"\").lower().startswith(file_prefix.lower())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e47756-e940-4c1b-996b-eaca0cd30e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 6. Obtener archivos\n",
    "# ===============================================\n",
    "#GET DATA FROM CONTROL TABLE\n",
    "\n",
    "query = \"SELECT * FROM \" + cfg.control_table + \" WHERE fileName = '\" + cfg.fin_var_cost_act_file_name + \"'\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "\n",
    "sourceFileNamePrefix = df.select('fileName').collect()[0][0]\n",
    "sourceFileExtension = df.select('fileExtension').collect()[0][0]\n",
    "sourceFileNameMask = df.select('fileNameMask').collect()[0][0]\n",
    "tableName = df.select('tableName').collect()[0][0]\n",
    "\n",
    "files_matched = []\n",
    "\n",
    "#Only get files matching the mask\n",
    "for f in files:\n",
    "    name = f.get(\"name\", \"\")\n",
    "    \n",
    "    if check_filename(sourceFileNameMask,sourceFileNamePrefix.lower(),sourceFileExtension.lower(),name.lower()):\n",
    "        files_matched.append(f)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0716de9d-a7be-455a-85f3-7602fbb72a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 7. Mover de Sharepoint a S3\n",
    "# ======================\n",
    "\n",
    "s3_bucket = cfg.raw_bucket \n",
    "s3_base_path = tableName\n",
    "\n",
    "logs = []\n",
    "\n",
    "# Inicializar cliente de S3\n",
    "\n",
    "# Obtiene la región por defecto del entorno\n",
    "session = boto3.session.Session()\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"aws_access_key_id\"),\n",
    "    aws_secret_access_key=dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"aws_secret_access_key\"),\n",
    "    region_name=session.region_name\n",
    ")\n",
    "\n",
    "for f in files_matched:\n",
    "    file_name = f.get(\"name\")\n",
    "    download_url = f.get(\"@microsoft.graph.downloadUrl\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not file_name or not download_url:\n",
    "        logs.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"target_path\": \"\",\n",
    "            \"status\": \"failed\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"error\": \"Invalid File\" \n",
    "        })\n",
    "        continue\n",
    "\n",
    "    target_file_name = f\"{file_name.rsplit('.',1)[0]}.{file_name.rsplit('.',1)[1]}\" #nombre original\n",
    "    target_s3_key = f\"{s3_base_path}/{target_file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Descargar archivo en memoria\n",
    "        response = requests.get(download_url)\n",
    "        response.raise_for_status()\n",
    "        file_bytes = response.content\n",
    "\n",
    "        # Subir a S3 directamente como binario\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=target_s3_key, Body=file_bytes)\n",
    "\n",
    "        logs.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"target_path\": f\"s3://{s3_bucket}/{target_s3_key}\",\n",
    "            \"status\": \"success\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"error\": \"\"\n",
    "        })\n",
    "        print(f\"✅ Archivo subido: s3://{s3_bucket}/{target_s3_key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logs.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"target_path\": f\"s3://{s3_bucket}/{target_s3_key}\",\n",
    "            \"status\": \"failed\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        print(f\"❌ Error Loading file {file_name}: {e}\")\n",
    "\n",
    "# Guardar log en S3 #PENDING poner log en librería\n",
    "log_df = pd.DataFrame(logs)\n",
    "csv_buffer = StringIO()\n",
    "log_df.to_csv(csv_buffer, index=False)\n",
    "s3_client.put_object(Bucket=s3_bucket, Key=f\"{s3_base_path}/upload_log.csv\", Body=csv_buffer.getvalue())\n",
    "print(f\"📄 Log guardado en s3://{s3_bucket}/{s3_base_path}/upload_log.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Finance - SP to S3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
