{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a178f270-4ab5-447e-b8c7-d4b083417982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esto es para que la librería se actualice automáticamente (el .py de los includes en este caso), en producción hay que tomar la decisión si debería quitarse para evitar cualquier problema y hacer un restart si se cambia la librería o si dejarlo y cuando se haga un cambio, lo va a tomar automáticamente\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d34753f-5f14-47ef-9d26-7713e1b0afd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import sys\n",
    "import importlib\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de cada carpeta\n",
    "from includes import control_functions\n",
    "from includes import validations\n",
    "from schema import fin_act_sch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8018f497-49b7-41d4-b4eb-c074da317b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#CREATE PYSPARK SESSION\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4fa199-1b97-4982-a783-6b671a3934c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG START\n",
    "process_setup_name = 'Load BI OVC'\n",
    "process_setup_step_name = 'bronze to silver'\n",
    "sys_modified_by_name = 'NBK - Load Finance - bronze to silver'\n",
    "source_system_code = 'sapbr'\n",
    "\n",
    "process_run_id = control_functions.log_process_run_start(process_setup_name,process_setup_step_name,source_system_code,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6fb491-c24e-40c7-913d-d8fb2d4b9383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET DATA FROM CONTROL TABLE\n",
    "df = control_functions.get_process_setup_parameters(process_setup_name,process_setup_step_name)\n",
    "\n",
    "sourceSchema = df.select('process_setup_source_data_definition').collect()[0][0]\n",
    "SourceBucket = df.select('process_setup_source_bucket_name').collect()[0][0]\n",
    "SourceBucketFolderKey = df.select('process_setup_source_bucket_folder_key').collect()[0][0]\n",
    "targetSchema = df.select('process_setup_target_data_definition').collect()[0][0]\n",
    "TargetBucket = df.select('process_setup_target_bucket_name').collect()[0][0]\n",
    "TargetBucketFolderKey = df.select('process_setup_target_bucket_folder_key').collect()[0][0]\n",
    "targetTable = df.select('process_setup_target_table_name').collect()[0][0]\n",
    "sourceTable = df.select('process_setup_source_table_name').collect()[0][0]\n",
    "sourceLayer = df.select('process_setup_source_layer').collect()[0][0]\n",
    "targetLayer = df.select('process_setup_target_layer').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9eee722-9b78-4e1f-a770-f159aef86c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ SCHEMAS - from .py\n",
    "src_schema = fin_act_sch.get_schema(sourceSchema)\n",
    "trgt_schema = fin_act_sch.get_schema(targetSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814864e2-590a-4583-bec3-a4a83ce01b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get current data from Bronze\n",
    "source_path = \"s3://\" + SourceBucket + \"/\" + SourceBucketFolderKey + \"/\"\n",
    "df_brz = []\n",
    "\n",
    "try:\n",
    "    # check file existense in DBFS\n",
    "    dbutils.fs.ls(source_path)\n",
    "\n",
    "    # read table\n",
    "    df_brz = spark.read.format(\"delta\").load(source_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ CAN'T READ DELTA {source_path}: {e}\")\n",
    "    raise\n",
    "\n",
    "df_brz = df_brz.where(F.col('DW_CURR_ROW_FLG') == F.lit(True)) #get current\n",
    "df_brz = df_brz.drop('DW_VALID_FROM_DT','DW_VALID_TO_DT') #drop this fields as in silver will have the current date\n",
    "df_brz = df_brz.withColumn(\"POSTING_MTH_ID\",F.date_format(F.to_date(\"POSTING_DATE\", \"yyyyMMdd\"), \"yyyyMM\"))\n",
    "\n",
    "#get last version available per month (this is a business definition)\n",
    "df_brz_last_version = df_brz.groupBy(\"POSTING_MTH_ID\").agg(\n",
    "    F.max(\"DW_FILE_NAME\").alias(\"DW_FILE_NAME\")\n",
    ")\n",
    "\n",
    "#display(df_brz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2f9a2c-2f58-4b35-a7e9-122ff71d6fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET LAST DATA AVAILABLE \n",
    "\n",
    "df_brz = (\n",
    "    df_brz.join(\n",
    "        df_brz_last_version,\n",
    "        on=[\"DW_FILE_NAME\", \"POSTING_MTH_ID\"],  # union key\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "#display(df_brz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f311db-e549-4660-a6c6-72f89307d357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de cada carpeta\n",
    "from includes import control_functions\n",
    "from includes import validations\n",
    "from schema import fin_act_sch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909b9746-8ee8-4ee4-b0e2-c27cd4f25940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#BS VALIDATIONS\n",
    "\n",
    "#Get all BS validations that applies for the current process\n",
    "df_validations = validations.get_object_validation(process_run_id, 'BS')\n",
    "\n",
    "#Validate and get records validated and records rejected\n",
    "df_validated, df_rejected = validations.business_validation(df_brz, df_validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5916e447-6333-4864-8794-09a9d1f32160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(df_rejected)\n",
    "display(df_validated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbdd7bd1-dc37-486b-8690-74ba58e41816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#WRITE IN SILVER\n",
    "\n",
    "if df_validated.limit(1).count() > 0:\n",
    "    target_path = \"s3://\" + TargetBucket + \"/\" + TargetBucketFolderKey + \"/\"\n",
    "\n",
    "    #check if target path exists, if not will create it\n",
    "    try:\n",
    "        dbutils.fs.ls(target_path)\n",
    "    except:\n",
    "        dbutils.fs.mkdirs(target_path)\n",
    "\n",
    "    df_silver = []\n",
    "\n",
    "    # check if target is an existing delta table, if not will create it\n",
    "    if DeltaTable.isDeltaTable(spark, target_path):\n",
    "\n",
    "        delta_table = DeltaTable.forPath(spark, target_path)\n",
    "\n",
    "        # get all current records from silver already loaded\n",
    "        df_silver = (\n",
    "            spark.read.format(\"delta\").load(target_path)\n",
    "            .filter(F.col('DW_CURR_ROW_FLG') == F.lit(True))\n",
    "            .drop('DW_VALID_FROM_DT','DW_VALID_TO_DT')\n",
    "        )\n",
    "        #generate Posting Month ID for reprocess\n",
    "        df_silver = df_silver.withColumn(\"POSTING_MTH_ID\",F.date_format(F.to_date(\"POSTING_DATE\", \"yyyyMMdd\"), \"yyyyMM\"))\n",
    "\n",
    "        #from the current, get those to be reprocessed to mark them as false\n",
    "        df_silver_reprocess = (\n",
    "            df_silver.join(\n",
    "                df_brz_last_version,\n",
    "                on=[\"POSTING_MTH_ID\"],  # key for union\n",
    "                how=\"inner\"\n",
    "            )\n",
    "        ).select(\"POSTING_MTH_ID\").distinct()\n",
    "        \n",
    "        #Mark records to reprocess as false\n",
    "        delta_table.alias(\"tgt\").merge(\n",
    "            df_silver_reprocess.alias(\"src\"),\n",
    "            \"tgt.POSTING_MTH_ID = src.POSTING_MTH_ID\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"tgt.DW_CURR_ROW_FLG = true\",\n",
    "            set={\n",
    "                \"DW_CURR_ROW_FLG\": F.lit(False),\n",
    "                \"DW_VALID_TO_DT\": F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")\n",
    "            }\n",
    "        ).execute()\n",
    "        \n",
    "        #Add DW columns to the validated data\n",
    "        df_validated.withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")) \\\n",
    "            .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\")) \\\n",
    "            .withColumn('DW_CURR_ROW_FLG', F.lit(True))\n",
    "\n",
    "        df_validated.write.format(\"delta\").mode(\"append\").save(target_path)\n",
    "\n",
    "    else:\n",
    "        #Create initial Delta table with DW columns\n",
    "        new_data = (\n",
    "            df_validated\n",
    "            .withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\"))\n",
    "            .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\"))\n",
    "            .withColumn('DW_CURR_ROW_FLG', F.lit(True))\n",
    "        )\n",
    "        # Grabar como nueva Delta Table\n",
    "        new_data.write.format(\"delta\").mode(\"append\").save(target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b4bdf8-db19-47bb-8f00-35a8624490f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#WRITE IN SILVER REJECTED\n",
    "\n",
    "if df_rejected.limit(1).count() > 0:\n",
    "    target_path = \"s3://\" + TargetBucket + \"-rejected\" + \"/\" + TargetBucketFolderKey + \"/\"\n",
    "\n",
    "    #check if target path exists, if not will create it\n",
    "    try:\n",
    "        dbutils.fs.ls(target_path)\n",
    "    except:\n",
    "        dbutils.fs.mkdirs(target_path)\n",
    "\n",
    "    df_silver = []\n",
    "\n",
    "    # check if target is an existing delta table, if not will create it\n",
    "    if DeltaTable.isDeltaTable(spark, target_path):\n",
    "\n",
    "        delta_table_rejected = DeltaTable.forPath(spark, target_path)\n",
    "\n",
    "        if \"DW_CURR_ROW_FLG\" in delta_table_rejected.toDF().columns:\n",
    "            delta_table_rejected.update(\n",
    "                condition=\"DW_CURR_ROW_FLG = true\",\n",
    "                set={\n",
    "                    \"DW_CURR_ROW_FLG\": \"false\",\n",
    "                    \"DW_VALID_TO_DT\": F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        #Add DW columns to the rejected data\n",
    "        df_rejected.withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")) \\\n",
    "            .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\")) \\\n",
    "            .withColumn('DW_CURR_ROW_FLG', F.lit(True))\n",
    "\n",
    "        df_rejected.write.format(\"delta\").mode(\"append\").save(target_path)\n",
    "\n",
    "    else:\n",
    "        #Create initial Delta table with DW columns\n",
    "        new_data = (\n",
    "            df_rejected\n",
    "            .withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\"))\n",
    "            .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\"))\n",
    "            .withColumn('DW_CURR_ROW_FLG', F.lit(True))\n",
    "        )\n",
    "        # Grabar como nueva Delta Table\n",
    "        new_data.write.format(\"delta\").mode(\"append\").save(target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d68cbf5-f130-4a8a-8ee9-3d858b3b00cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#table = \"`latam-md-finance`.silver_rejected.tb_fin_variable_cost_act\" \n",
    "#spark.sql(\"DROP TABLE IF EXISTS \" + table + \"\")\n",
    "#spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table + \" USING DELTA LOCATION 's3a://latam-md-finance-silver-rejected/tb_fin_variable_cost_act'\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8585516552376328,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Finance - bronze to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
