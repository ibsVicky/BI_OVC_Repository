{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b534a64-1263-4607-9946-b61af176eab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esto es para que la librer√≠a se actualice autom√°ticamente (el .py de los includes en este caso), en producci√≥n hay que tomar la decisi√≥n si deber√≠a quitarse para evitar cualquier problema y hacer un restart si se cambia la librer√≠a o si dejarlo y cuando se haga un cambio, lo va a tomar autom√°ticamente\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be0aaab-7cf0-4c2b-b77a-52c260819f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install msal requests #PENDING esto deber√≠a ir en el cluster\n",
    "%pip install boto3 #PENDING esto deber√≠a ir en el cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c1a607-f131-48ff-8372-ab88f69b04cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from msal import ConfidentialClientApplication\n",
    "from datetime import datetime\n",
    "from io import StringIO, BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de includes y de config\n",
    "from includes.file_functions import check_filename, imprimir \n",
    "import config.config as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c03aa4-6b9e-4e2c-90c3-f845e8ea6a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbf6ae9-2a63-4b32-8081-1cf4cf37f19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. Configuraci√≥n\n",
    "# ======================\n",
    "\n",
    "tenant_id = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"tenant_id\")\n",
    "client_id = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"client_secret\")\n",
    "\n",
    "hostname = cfg.hostname\n",
    "site_relative = cfg.site_relative\n",
    "\n",
    "# ======================\n",
    "# 2. Obtener token (Bearer)\n",
    "# ======================\n",
    "authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "scope = [\"https://graph.microsoft.com/.default\"]\n",
    "\n",
    "app = ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
    "token = app.acquire_token_for_client(scopes=scope)\n",
    "\n",
    "if \"access_token\" not in token: \n",
    "    raise Exception(f\"Error getting token: {token}\") \n",
    "access_token = token[\"access_token\"]\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "# ======================\n",
    "# 3. Obtener siteId\n",
    "# ======================\n",
    "\n",
    "url_site = f\"https://graph.microsoft.com/v1.0/sites/{hostname}:/{site_relative}\"\n",
    "r = requests.get(url_site, headers=headers)\n",
    "\n",
    "#print(json.dumps(r, indent=2))\n",
    "\n",
    "site = r.json()\n",
    "site_id = site.get(\"id\")\n",
    "\n",
    "# ======================\n",
    "# 4. Obtener drives y elegir \"Documents\"\n",
    "# ======================\n",
    "url_drives = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives\"\n",
    "drives_response = requests.get(url_drives, headers=headers).json()\n",
    "\n",
    "if 'value' not in drives_response:\n",
    "    raise Exception(f\"Error fetching drives: {drives_response}\")\n",
    "drives = drives_response\n",
    "\n",
    "drive_id = next(d[\"id\"] for d in drives[\"value\"] if d[\"name\"] == cfg.sp_file_folder)\n",
    "\n",
    "# ======================\n",
    "# 5. Obtener archivos en carpeta\n",
    "# ======================\n",
    "\n",
    "folder = \"BI_OVC_TST\" #PENDING charlar dejar ac√° o m√°s arriba, porque solo se usa aqu√≠\n",
    "url_children = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/root:/{folder}:/children\"\n",
    "resp = requests.get(url_children, headers=headers).json()\n",
    "\n",
    "file_prefix = cfg.fin_var_cost_act_file_name\n",
    "files = [f for f in resp.get(\"value\", []) if f.get(\"name\", \"\").lower().startswith(file_prefix.lower())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e47756-e940-4c1b-996b-eaca0cd30e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 6. Obtener archivos\n",
    "# ===============================================\n",
    "#GET DATA FROM CONTROL TABLE\n",
    "\n",
    "query = \"SELECT * FROM \" + cfg.control_table + \" WHERE fileName = '\" + cfg.fin_var_cost_act_file_name + \"'\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "\n",
    "sourceFileNamePrefix = df.select('fileName').collect()[0][0]\n",
    "sourceFileExtension = df.select('fileExtension').collect()[0][0]\n",
    "sourceFileNameMask = df.select('fileNameMask').collect()[0][0]\n",
    "tableName = df.select('tableName').collect()[0][0]\n",
    "\n",
    "files_matched = []\n",
    "\n",
    "#Only get files matching the mask\n",
    "for f in files:\n",
    "    name = f.get(\"name\", \"\")\n",
    "    \n",
    "    if check_filename(sourceFileNameMask,sourceFileNamePrefix.lower(),sourceFileExtension.lower(),name.lower()):\n",
    "        files_matched.append(f)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0716de9d-a7be-455a-85f3-7602fbb72a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 7. Mover de Sharepoint a S3\n",
    "# ======================\n",
    "\n",
    "s3_bucket = cfg.raw_bucket \n",
    "s3_base_path = tableName\n",
    "\n",
    "logs = []\n",
    "\n",
    "# Inicializar cliente de S3\n",
    "\n",
    "# Obtiene la regi√≥n por defecto del entorno\n",
    "session = boto3.session.Session()\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"aws_access_key_id\"),\n",
    "    aws_secret_access_key=dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"aws_secret_access_key\"),\n",
    "    region_name=session.region_name\n",
    ")\n",
    "\n",
    "for f in files_matched:\n",
    "    file_name = f.get(\"name\")\n",
    "    download_url = f.get(\"@microsoft.graph.downloadUrl\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not file_name or not download_url:\n",
    "        logs.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"target_path\": \"\",\n",
    "            \"status\": \"failed\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"error\": \"Invalid File\" \n",
    "        })\n",
    "        continue\n",
    "\n",
    "    target_file_name = f\"{file_name.rsplit('.',1)[0]}.{file_name.rsplit('.',1)[1]}\" #nombre original\n",
    "    target_s3_key = f\"{s3_base_path}/{target_file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Descargar archivo en memoria\n",
    "        response = requests.get(download_url)\n",
    "        response.raise_for_status()\n",
    "        file_bytes = response.content\n",
    "\n",
    "        # Subir a S3 directamente como binario\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=target_s3_key, Body=file_bytes)\n",
    "\n",
    "        logs.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"target_path\": f\"s3://{s3_bucket}/{target_s3_key}\",\n",
    "            \"status\": \"success\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"error\": \"\"\n",
    "        })\n",
    "        print(f\"‚úÖ Archivo subido: s3://{s3_bucket}/{target_s3_key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logs.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"target_path\": f\"s3://{s3_bucket}/{target_s3_key}\",\n",
    "            \"status\": \"failed\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        print(f\"‚ùå Error Loading file {file_name}: {e}\")\n",
    "\n",
    "# Guardar log en S3 #PENDING poner log en librer√≠a\n",
    "log_df = pd.DataFrame(logs)\n",
    "csv_buffer = StringIO()\n",
    "log_df.to_csv(csv_buffer, index=False)\n",
    "s3_client.put_object(Bucket=s3_bucket, Key=f\"{s3_base_path}/upload_log.csv\", Body=csv_buffer.getvalue())\n",
    "print(f\"üìÑ Log guardado en s3://{s3_bucket}/{s3_base_path}/upload_log.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Finance - SP to S3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
