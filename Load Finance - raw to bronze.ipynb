{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c1a607-f131-48ff-8372-ab88f69b04cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import sys\n",
    "import importlib\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "from includes import file_functions #esto es funciona porque existe un __init__.py dentro de includes\n",
    "from config import config #esto es el config.py de la carpeta config, funciona porque existe un __init__.py\n",
    "from schema import fin_act_sch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c03aa4-6b9e-4e2c-90c3-f845e8ea6a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "079041b5-ca9e-4eb7-bf42-3203806a4902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET DATA FROM CONTROL TABLE\n",
    "\n",
    "query = \"SELECT * FROM \" + config.control_table + \" WHERE fileName = '\" + config.fin_var_cost_act_file_name + \"'\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "\n",
    "sourceFileNamePrefix = df.select('fileName').collect()[0][0]\n",
    "sourceFileExtension = df.select('fileExtension').collect()[0][0]\n",
    "sourceFileDelimiter = df.select('fileDelimiter').collect()[0][0]\n",
    "sourceFileEncoding = df.select('fileEncoding').collect()[0][0]\n",
    "sourceSchema = df.select('fileSchema').collect()[0][0]\n",
    "sourceFileNameMask = df.select('fileNameMask').collect()[0][0]\n",
    "tableName = df.select('tableName').collect()[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fafbec-5e16-4646-9400-14d37bdadf59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ SCHEMAS - from .py\n",
    "raw_schema = fin_act_sch.get_schema(sourceSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3df8ac3-993b-4640-81bd-29ec759f037c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET FILE NAMES FROM RAW SOURCE\n",
    "files_to_process = []\n",
    "\n",
    "raw_generic_path = \"s3://\" + config.raw_bucket + \"/\" + tableName + \"/\"\n",
    "\n",
    "files = dbutils.fs.ls(raw_generic_path)\n",
    "\n",
    "files_to_process = [\n",
    "    f for f in files\n",
    "    if f.name.lower().startswith(sourceFileNamePrefix.lower()) and f.name.lower().endswith(f\".{sourceFileExtension.lower()}\")\n",
    "]\n",
    "\n",
    "files_to_process.sort()\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"No matching files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3b58a9-9618-416e-8b5e-5a0cf3e839b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Only process the newest file (list sorted), the rest goes directly to archive\n",
    "archive_path = \"s3://\" + config.archive_bucket + \"/\" + tableName\n",
    "bronze_generic_path = \"s3://\" + config.bronze_bucket + \"/\" \n",
    "\n",
    "if files_to_process:\n",
    "    for i, file in enumerate(files_to_process, start=1):\n",
    "\n",
    "        #CHECK FILENAME\n",
    "\n",
    "        #READ SOURCE FILE AND ADD COLUMN NAMES TO DATAFRAME\n",
    "\n",
    "        file_path = raw_generic_path + file.name\n",
    "\n",
    "        try: \n",
    "            df_raw = spark.read.options(encoding=sourceFileEncoding,delimiter=sourceFileDelimiter, header=False, schema=raw_schema).csv(file_path) \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        df_raw = df_raw.toDF(*[f.name for f in raw_schema.fields])\n",
    "        source_cnt = df_raw.count()\n",
    "\n",
    "        #WRITE IN BRONZE\n",
    "        if df_raw and df_raw.limit(1).count() > 0:\n",
    "            bronze_delta_path = bronze_generic_path.rstrip(\"/\") + \"/\" + tableName\n",
    "\n",
    "            try:\n",
    "                dbutils.fs.ls(bronze_delta_path)\n",
    "            except:\n",
    "                #dbutils.fs.mkdirs(bronze_delta_path)\n",
    "                df_empty = spark.createDataFrame([], \"DW_VALID_FROM_DT timestamp\")  # ajusta el esquema\n",
    "                df_empty.write.format(\"delta\").save(bronze_delta_path)\n",
    "                \n",
    "            try:\n",
    "                # Enriquecer el DF SIEMPRE antes de escribir\n",
    "                df_raw = df_raw.withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")) \\\n",
    "                    .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn('DW_CURR_ROW_FLG', F.lit(True)) \\\n",
    "                    .withColumn('DW_FILE_NAME', F.lit(file.name))\n",
    "                \n",
    "                if DeltaTable.isDeltaTable(spark, bronze_delta_path):\n",
    "                    delta_table = DeltaTable.forPath(spark, bronze_delta_path)\n",
    "                    \n",
    "                    # Actualizar registros existentes (solo en la primera vuelta, para que no marque false los registros de otros files insertados en la misma corrida)\n",
    "                    if i == 1 and \"DW_CURR_ROW_FLG\" in delta_table.toDF().columns:\n",
    "                        delta_table.update(\n",
    "                            condition=\"DW_CURR_ROW_FLG = true\",\n",
    "                            set={\n",
    "                                \"DW_CURR_ROW_FLG\": \"false\",\n",
    "                                \"DW_VALID_TO_DT\": F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")\n",
    "                            }\n",
    "                        )\n",
    "                    \n",
    "                    mode = \"append\"\n",
    "                else:\n",
    "                    mode = \"overwrite\"  # primera carga\n",
    "                \n",
    "                # DataFrame mínimo solo con esquema\n",
    "\n",
    "                df_raw.write.format(\"delta\").mode(mode).option(\"mergeSchema\", \"true\").save(bronze_delta_path)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error writing file {bronze_generic_path}: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(\"DataFrame vacío, no se sobrescribió el bronze.\")\n",
    "\n",
    "        target_cnt = df_raw.count()\n",
    "\n",
    "        #Always move to archive\n",
    "        source_path = raw_generic_path.rstrip(\"/\")\n",
    "        target_path = archive_path.rstrip(\"/\")\n",
    "\n",
    "        # Ruta completa del archivo origen\n",
    "        source_file_full = f\"{source_path}/{file.name}\"\n",
    "\n",
    "        # Verificar existencia\n",
    "        try:\n",
    "            files = [f.name for f in dbutils.fs.ls(source_path)]\n",
    "            if file.name not in files:\n",
    "                print(f\"Archivo no encontrado: {source_file_full}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al acceder al path origen: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Crear carpeta destino si no existe\n",
    "        try:\n",
    "            dbutils.fs.ls(target_path)\n",
    "        except:\n",
    "            dbutils.fs.mkdirs(target_path)\n",
    "\n",
    "        # Generar nombre con timestamp\n",
    "        #source_file_name, source_file_extension = file.name.split(\".\", 1)\n",
    "        #today = datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        #target_file_name = f\"{source_file_name}_{today}.{source_file_extension}\"\n",
    "        target_file_name = file.name\n",
    "\n",
    "        # Rutas completas\n",
    "        target_file_full = f\"{target_path}/{target_file_name}\"\n",
    "\n",
    "        # Copiar y borrar\n",
    "        dbutils.fs.cp(source_file_full, target_file_full)\n",
    "        dbutils.fs.rm(source_file_full)\n",
    "\n",
    "        print(f\"Archivo movido: {source_file_full} → {target_file_full}\")\n",
    "else:\n",
    "    print(\"No files to process\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Finance - raw to bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
