{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b534a64-1263-4607-9946-b61af176eab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esto es para que la librería se actualice automáticamente (el .py de los includes en este caso), en producción hay que tomar la decisión si debería quitarse para evitar cualquier problema y hacer un restart si se cambia la librería o si dejarlo y cuando se haga un cambio, lo va a tomar automáticamente\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be0aaab-7cf0-4c2b-b77a-52c260819f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install msal requests #PENDING esto debería ir en el cluster\n",
    "%pip install boto3 #PENDING esto debería ir en el cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c1a607-f131-48ff-8372-ab88f69b04cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from msal import ConfidentialClientApplication\n",
    "from datetime import datetime\n",
    "from io import StringIO, BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de includes y de config\n",
    "from includes.file_functions import check_filename, imprimir \n",
    "from includes import control_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c03aa4-6b9e-4e2c-90c3-f845e8ea6a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde856f4-f240-48b8-b323-a14d9cea967f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GENERIC PARAMETER\n",
    "sys_status_column_name = 'sys_status_code'\n",
    "error_status_code = 'E'\n",
    "fn_status = True\n",
    "process_source_name = 'Load BI OVC'\n",
    "process_step_name = 'sharepoint to s3'\n",
    "sys_modified_by_name = 'NBK - Load Finance - SP to S3'\n",
    "source_system_code = 'SAPBR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ad7baf-3d1d-4396-be1d-a6c549663a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG START\n",
    "process_run_id, fn_status = control_functions.log_process_run_start(process_source_name,process_step_name,source_system_code,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f3ba17-f707-41b3-a60d-5e4ab645f8d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IF errors, set row status to E\n",
    "if process_run_id > 0:\n",
    "  control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "\n",
    "if fn_status == False:\n",
    "  print(f\"❌ ERROR STARTING PROCESS\")\n",
    "  raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61a8938-ca25-4b00-aa51-47a6ab5e57f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#FILE PARAMETERS FROM NOTEBOOK\n",
    "source_file_extension = 'txt'\n",
    "source_file_name_mask = '{fileName}_{yyyymmdd}_{hhmmss}.{fileExtension}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625e0652-4f15-4b59-b695-c4302462c938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET DATA FROM CONTROL TABLE\n",
    "try:\n",
    "    df = control_functions.get_process_source_parameters(process_source_name,process_step_name)\n",
    "except Exception as e:\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n",
    "    print(f\"❌ ERROR GETTING PROCESS SETUP PARAMETERS: {e}\")\n",
    "    raise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d71355c-f16d-422a-a03e-5dcbb6070ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PARAMETERS FROM CONTROL TABLE\n",
    "source_file_name_prefix = df.select('source_file_name_prefix').collect()[0][0]\n",
    "\n",
    "sharepoint_hostname = df.select('source_sharepoint_host_name').collect()[0][0]\n",
    "source_sharepoint_site_relative = df.select('source_sharepoint_site_relative').collect()[0][0]\n",
    "source_sharepoint_drive = df.select('source_sharepoint_drive').collect()[0][0]\n",
    "source_sharepoint_file_path = df.select('source_sharepoint_file_path').collect()[0][0]\n",
    "\n",
    "raw_bucket = df.select('target_bucket_name').collect()[0][0]\n",
    "bucketFolderKey = df.select('target_bucket_folder_key').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbf6ae9-2a63-4b32-8081-1cf4cf37f19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. Configuration\n",
    "# ======================\n",
    "\n",
    "tenant_id = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"tenant_id\")\n",
    "client_id = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"client_secret\")\n",
    "\n",
    "# ======================\n",
    "# 2. Get token (Bearer)\n",
    "# ======================\n",
    "authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "scope = [\"https://graph.microsoft.com/.default\"]\n",
    "\n",
    "app = ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
    "token = app.acquire_token_for_client(scopes=scope)\n",
    "\n",
    "if \"access_token\" not in token: \n",
    "    raise Exception(f\"Error getting token: {token}\") \n",
    "access_token = token[\"access_token\"]\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "# ======================\n",
    "# 3. Get siteId\n",
    "# ======================\n",
    "\n",
    "url_site = f\"https://graph.microsoft.com/v1.0/sites/{sharepoint_hostname}:/{source_sharepoint_site_relative}\"\n",
    "r = requests.get(url_site, headers=headers)\n",
    "\n",
    "#print(json.dumps(r, indent=2))\n",
    "\n",
    "site = r.json()\n",
    "site_id = site.get(\"id\")\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Get drives and pick the requested one (source_sharepoint_drive)\n",
    "# =========================================================================\n",
    "\n",
    "url_drives = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives\"\n",
    "drives_response = requests.get(url_drives, headers=headers).json()\n",
    "\n",
    "if 'value' not in drives_response:\n",
    "    raise Exception(f\"Error fetching drives: {drives_response}\")\n",
    "drives = drives_response\n",
    "\n",
    "drive_id = next(d[\"id\"] for d in drives[\"value\"] if d[\"name\"] == source_sharepoint_drive)\n",
    "\n",
    "# =======================\n",
    "# 5. Get files in Folder\n",
    "# =======================\n",
    "\n",
    "url_children = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/root:/{source_sharepoint_file_path}:/children\"\n",
    "resp = requests.get(url_children, headers=headers).json()\n",
    "\n",
    "file_prefix = source_file_name_prefix\n",
    "files = [f for f in resp.get(\"value\", []) if f.get(\"name\", \"\").lower().startswith(file_prefix.lower())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e47756-e940-4c1b-996b-eaca0cd30e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 6. Get files matching the mask\n",
    "# ===============================================\n",
    "\n",
    "files_matched = []\n",
    "\n",
    "#Only get files matching the mask\n",
    "for f in files:\n",
    "    name = f.get(\"name\", \"\")\n",
    "\n",
    "    if check_filename(source_file_name_mask,source_file_name_prefix.lower(),source_file_extension.lower(),name.lower()):\n",
    "        files_matched.append(f)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0716de9d-a7be-455a-85f3-7602fbb72a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 7. Move from Sharepoint to S3\n",
    "# ================================\n",
    "\n",
    "s3_bucket = raw_bucket \n",
    "s3_base_path = bucketFolderKey\n",
    "\n",
    "logs = []\n",
    "\n",
    "# Initialize S3 client\n",
    "\n",
    "# Get environment region\n",
    "session = boto3.session.Session()\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"aws_access_key_id\"),\n",
    "    aws_secret_access_key=dbutils.secrets.get(scope = \"ibs-sharepoint-databricks-secret\", key = \"aws_secret_access_key\"),\n",
    "    region_name=session.region_name\n",
    ")\n",
    "\n",
    "for f in files_matched:\n",
    "    file_name = f.get(\"name\")\n",
    "    download_url = f.get(\"@microsoft.graph.downloadUrl\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not file_name or not download_url:\n",
    "        print(f\"⚠️ FILE NOT LOADED - PLEASE CHECK: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    target_file_name = f\"{file_name.rsplit('.',1)[0]}.{file_name.rsplit('.',1)[1]}\" #original name\n",
    "    target_s3_key = f\"{s3_base_path}/{target_file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Download file in memory\n",
    "        response = requests.get(download_url)\n",
    "        response.raise_for_status()\n",
    "        file_bytes = response.content\n",
    "\n",
    "        # Upload to S3 as binary\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=target_s3_key, Body=file_bytes)\n",
    "        print(f\"✅ File Loaded: s3://{s3_bucket}/{target_s3_key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR LOADING FILE {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead7d09b-7274-442e-943a-b1f262f48632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG END\n",
    "x = control_functions.log_process_run_end(process_run_id,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8cb9b6a-36ce-40ca-8277-6ca03831bfe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IF errors, set row status to E\n",
    "if x == False:\n",
    "  control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,sys_status_column_name, error_status_code)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Finance - SP to S3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
