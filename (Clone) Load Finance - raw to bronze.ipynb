{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "495c8770-7e17-490c-b6d4-c3701ce26fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esto es para que la librería se actualice automáticamente (el .py de los includes en este caso), en producción hay que tomar la decisión si debería quitarse para evitar cualquier problema y hacer un restart si se cambia la librería o si dejarlo y cuando se haga un cambio, lo va a tomar automáticamente\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c1a607-f131-48ff-8372-ab88f69b04cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import sys\n",
    "import importlib\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "sys.path.append(\"/Workspace/BI-OVC\")\n",
    "\n",
    "#esto es funciona porque existe un __init__.py dentro de includes y de config\n",
    "from includes import file_functions \n",
    "from includes import control_functions\n",
    "from schema import fin_act_sch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c03aa4-6b9e-4e2c-90c3-f845e8ea6a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bi-ovc-test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1fb259-1a50-4673-a3b2-b1502e495e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG START\n",
    "process_setup_name = 'Load BI OVC'\n",
    "process_setup_step_name = 'raw to bronze'\n",
    "sys_modified_by_name = 'NBK - Load Finance - raw to bronze'\n",
    "\n",
    "process_run_id = control_functions.log_process_run_start(process_setup_name,process_setup_step_name,sys_modified_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "079041b5-ca9e-4eb7-bf42-3203806a4902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET DATA FROM CONTROL TABLE\n",
    "df = control_functions.get_process_setup_parameters(process_setup_name,process_setup_step_name)\n",
    "\n",
    "sourceFileNamePrefix = df.select('process_setup_source_file_name').collect()[0][0]\n",
    "sourceFileExtension = df.select('process_setup_source_file_extension').collect()[0][0]\n",
    "sourceFileDelimiter = df.select('process_setup_source_file_delimiter').collect()[0][0]\n",
    "sourceFileEncoding = df.select('process_setup_source_file_encoding').collect()[0][0]\n",
    "sourceFileNameMask = df.select('process_setup_source_file_name_mask').collect()[0][0]\n",
    "sourceSchema = df.select('process_setup_source_file_schema').collect()[0][0]\n",
    "SourceBucket = df.select('process_setup_source_bucket_name').collect()[0][0]\n",
    "SourceBucketFolderKey = df.select('process_setup_target_bucket_folder_key').collect()[0][0]\n",
    "TargetBucket = df.select('process_setup_target_bucket_name').collect()[0][0]\n",
    "TargetBucketFolderKey = df.select('process_setup_target_bucket_folder_key').collect()[0][0]\n",
    "tableName = df.select('process_setup_target_table_name').collect()[0][0]\n",
    "ArchiveBucket = df.select('process_setup_archive_bucket_name').collect()[0][0]\n",
    "ArchiveBucketFolderKey = df.select('process_setup_archive_bucket_folder_key').collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fafbec-5e16-4646-9400-14d37bdadf59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ SCHEMAS - from .py\n",
    "raw_schema = fin_act_sch.get_schema(sourceSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3df8ac3-993b-4640-81bd-29ec759f037c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GET FILE NAMES FROM RAW SOURCE\n",
    "files_to_process = []\n",
    "\n",
    "raw_generic_path = \"s3://\" + SourceBucket + \"/\" + SourceBucketFolderKey + \"/\"\n",
    "\n",
    "files = dbutils.fs.ls(raw_generic_path)\n",
    "\n",
    "files_to_process = [\n",
    "    f for f in files\n",
    "    if f.name.lower().startswith(sourceFileNamePrefix.lower()) and f.name.lower().endswith(f\".{sourceFileExtension.lower()}\")\n",
    "]\n",
    "\n",
    "files_to_process.sort()\n",
    "\n",
    "if not files_to_process:\n",
    "    print(f\"⚠️ NO MATCHING FILES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3b58a9-9618-416e-8b5e-5a0cf3e839b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "archive_path = \"s3://\" + ArchiveBucket + \"/\" + ArchiveBucketFolderKey\n",
    "bronze_generic_path = \"s3://\" + TargetBucket + \"/\" \n",
    "\n",
    "source_cnt = 0\n",
    "target_cnt = 0\n",
    "curr_row_flg_updated = 0 \n",
    "\n",
    "if files_to_process:\n",
    "    for i, file in enumerate(files_to_process, start=1):\n",
    "        #CHECK FILENAME\n",
    "\n",
    "        #READ SOURCE FILE AND ADD COLUMN NAMES TO DATAFRAME\n",
    "        file_path = raw_generic_path + file.name\n",
    "\n",
    "        try: \n",
    "            df_raw = spark.read.options(encoding=sourceFileEncoding,delimiter=sourceFileDelimiter, header=False, schema=raw_schema).csv(file_path) \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR READING FILE {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        #Rename columns using schema names\n",
    "        df_raw = df_raw.toDF(*[f.name for f in raw_schema.fields])\n",
    "\n",
    "        #Accumulate count of rows from source before any validation, count inside loop in case of many files loaded\n",
    "        source_cnt += df_raw.count()\n",
    "\n",
    "        #PH VALIDATIONS\n",
    "\n",
    "        #Get all validations that applies for the current process\n",
    "        df_validations = control_functions.get_object_validation(process_run_id, '')\n",
    "\n",
    "        # For each validation, build a condition for each type\n",
    "        conditions = []\n",
    "\n",
    "        for validation in df_validations.collect():\n",
    "            col_name = validation['object_name']\n",
    "            rule_type = validation['validation_rule']\n",
    "            rule_detail = validation['validation_rule_detail']\n",
    "            \n",
    "            if rule_type == 'date_format':\n",
    "                conditions.append(F.expr(f\"try_to_date({col_name}, '{rule_detail}') is not null\") | F.col(col_name).isNull() )\n",
    "            elif rule_type == 'numeric_format':\n",
    "                conditions.append(F.expr(f\"try_cast({col_name} as {rule_detail}) is not null\") | F.col(col_name).isNull() )\n",
    "            elif rule_type == 'not_null':\n",
    "                conditions.append(F.col(col_name).isNotNull())\n",
    "\n",
    "        # combine all conditions in one\n",
    "        cond_bronze = conditions[0]\n",
    "\n",
    "        for condition in conditions[1:]:\n",
    "            cond_bronze = cond_bronze & condition\n",
    "\n",
    "        #get all records that pass the validation\n",
    "        df_bronze = df_raw.filter(cond_bronze)\n",
    "\n",
    "        #get all records that dont pass the validation\n",
    "        df_bronze_rejected = df_raw.filter(~cond_bronze)\n",
    "\n",
    "        #WRITE IN BRONZE\n",
    "        #if df_bronze and df_bronze.limit(1).count() > 0:\n",
    "        if df_bronze.limit(1).count() > 0:\n",
    "            bronze_delta_path = bronze_generic_path.rstrip(\"/\") + \"/\" + tableName\n",
    "\n",
    "            try:\n",
    "                dbutils.fs.ls(bronze_delta_path)\n",
    "            except:\n",
    "                #dbutils.fs.mkdirs(bronze_delta_path)\n",
    "                df_empty = spark.createDataFrame([], \"DW_VALID_FROM_DT timestamp\")  # ajusta el esquema\n",
    "                df_empty.write.format(\"delta\").save(bronze_delta_path)\n",
    "                \n",
    "            try:\n",
    "                # Enrich DF ALWAYS before writing\n",
    "                df_bronze = df_bronze.withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")) \\\n",
    "                    .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn('DW_CURR_ROW_FLG', F.lit(True)) \\\n",
    "                    .withColumn('DW_FILE_NAME', F.lit(file.name))\n",
    "                \n",
    "                if DeltaTable.isDeltaTable(spark, bronze_delta_path):\n",
    "                    delta_table = DeltaTable.forPath(spark, bronze_delta_path)\n",
    "                    \n",
    "                    # Update existing records (only the first time (i==1), so doesnt mark as False the records of other files inserted in the same process running).\n",
    "                    if i == 1 and curr_row_flg_updated == 0 and \"DW_CURR_ROW_FLG\" in delta_table.toDF().columns:\n",
    "                        delta_table.update(\n",
    "                            condition=\"DW_CURR_ROW_FLG = true\",\n",
    "                            set={\n",
    "                                \"DW_CURR_ROW_FLG\": \"false\",\n",
    "                                \"DW_VALID_TO_DT\": F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        curr_row_flg_updated == 1 \n",
    "                    mode = \"append\"\n",
    "                else:\n",
    "                    mode = \"overwrite\"  #first loading\n",
    "                \n",
    "                # DataFrame with only the schema\n",
    "                df_bronze.write.format(\"delta\").mode(mode).option(\"mergeSchema\", \"true\").save(bronze_delta_path)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"❌ ERROR READING FILE {bronze_generic_path}: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"⚠️ EMPTY DATAFRAME\")\n",
    "\n",
    "        #WRITE IN BRONZE_REJECTED\n",
    "        #if df_bronze_rejected and df_bronze_rejected.limit(1).count() > 0:\n",
    "        df_empty = []\n",
    "        if df_bronze_rejected.limit(1).count() > 0:\n",
    "            bronze_delta_path = bronze_generic_path.rstrip(\"/\") + \"-rejected\" + \"/\" + tableName \n",
    "\n",
    "            try:\n",
    "                dbutils.fs.ls(bronze_delta_path)\n",
    "            except:\n",
    "                df_empty = spark.createDataFrame([], \"DW_VALID_FROM_DT timestamp\")  # ajusta el esquema\n",
    "                df_empty.write.format(\"delta\").save(bronze_delta_path)\n",
    "                \n",
    "            try:\n",
    "                # Enrich DF ALWAYS before writing\n",
    "                df_bronze_rejected = df_bronze_rejected.withColumn('DW_VALID_FROM_DT', F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")) \\\n",
    "                    .withColumn('DW_VALID_TO_DT', F.lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn('DW_CURR_ROW_FLG', F.lit(True)) \\\n",
    "                    .withColumn('DW_FILE_NAME', F.lit(file.name))\n",
    "                \n",
    "                if DeltaTable.isDeltaTable(spark, bronze_delta_path):\n",
    "                    delta_table_rejected = DeltaTable.forPath(spark, bronze_delta_path)\n",
    "                    \n",
    "                    # Update existing records (only the first time (i==1), so doesnt mark as False the records of other files inserted in the same process running) And if it wasn't updated for Write in Bronze first\n",
    "\n",
    "                    if i == 1 and curr_row_flg_updated == 0 and \"DW_CURR_ROW_FLG\" in delta_table_rejected.toDF().columns:\n",
    "                        delta_table_rejected.update(\n",
    "                            condition=\"DW_CURR_ROW_FLG = true\",\n",
    "                            set={\n",
    "                                \"DW_CURR_ROW_FLG\": \"false\",\n",
    "                                \"DW_VALID_TO_DT\": F.from_utc_timestamp(F.current_timestamp(), \"Brazil/East\")\n",
    "                            }\n",
    "                        )\n",
    "                    \n",
    "                    curr_row_flg_updated == 1 \n",
    "                    \n",
    "                    mode = \"append\"\n",
    "                else:\n",
    "                    mode = \"overwrite\"  # primera carga\n",
    "                \n",
    "                # DataFrame mínimo solo con esquema\n",
    "\n",
    "                df_bronze_rejected.write.format(\"delta\").mode(mode).option(\"mergeSchema\", \"true\").save(bronze_delta_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing file {bronze_generic_path}: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(\"DataFrame vacío, no se sobrescribió el bronze.\")\n",
    "\n",
    "        #Accumulate count of rows moved to target (not rejected), count inside loop in case of many files loaded\n",
    "        target_cnt += df_bronze.count()\n",
    "\n",
    "        #Always move to archive\n",
    "        source_path = raw_generic_path.rstrip(\"/\")\n",
    "        target_path = archive_path.rstrip(\"/\")\n",
    "\n",
    "        # Ruta completa del archivo origen\n",
    "        source_file_full = f\"{source_path}/{file.name}\"\n",
    "\n",
    "        # Verificar existencia\n",
    "        try:\n",
    "            files = [f.name for f in dbutils.fs.ls(source_path)]\n",
    "            if file.name not in files:\n",
    "                print(f\"Archivo no encontrado: {source_file_full}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al acceder al path origen: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Crear carpeta destino si no existe\n",
    "        try:\n",
    "            dbutils.fs.ls(target_path)\n",
    "        except:\n",
    "            dbutils.fs.mkdirs(target_path)\n",
    "\n",
    "        # Generar nombre con timestamp\n",
    "        target_file_name = file.name\n",
    "\n",
    "        # Rutas completas\n",
    "        target_file_full = f\"{target_path}/{target_file_name}\"\n",
    "\n",
    "        # Copiar y borrar\n",
    "        dbutils.fs.cp(source_file_full, target_file_full)\n",
    "        dbutils.fs.rm(source_file_full)\n",
    "\n",
    "        print(f\"Archivo movido: {source_file_full} → {target_file_full}\")\n",
    "\n",
    "    \n",
    "    #LOG SOURCE & TARGET RECORD COUNT\n",
    "    #update outside of the loop\n",
    "    \n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,'process_run_source_record_count', source_cnt)\n",
    "\n",
    "    control_functions.log_process_run_update_value(process_run_id,sys_modified_by_name,'process_run_target_record_count', target_cnt)\n",
    "else:\n",
    "    print(f\"⚠️ NO FILES TO PROCESS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45346a66-9d16-4291-bba7-b705d1baa8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#table = \"`latam-md-finance`.bronze_rejected.tb_fin_variable_cost_act\" \n",
    "#spark.sql(\"DROP TABLE IF EXISTS \" + table + \"\")\n",
    "#spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table + \" USING DELTA LOCATION 's3a://latam-md-finance-bronze-rejected/tb_fin_variable_cost_act'\")\n",
    "\n",
    "#print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae1a3c4-3864-468e-af51-762677338721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LOG END\n",
    "control_functions.log_process_run_end(process_run_id,sys_modified_by_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5174651618437435,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Load Finance - raw to bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
